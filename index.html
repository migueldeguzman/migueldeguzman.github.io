<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Portfolio</title>
    <link rel="icon" href="https://github.com/migueldeguzman/migueldeguzman.github.io/image.png" type="image/x-icon">
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 40px auto;
            max-width: 800px;
            line-height: 1.6;
            padding: 0 20px; /* Add padding to prevent leaning against the edges */
        }

        header {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 10px 0;
        }

        section {
            margin: 20px 0;
        }

        img {
            max-width: 100%;
        }

        footer {
            text-align: center;
            margin-top: 40px;
            font-size: 0.8em;
            color: #777;
        }

        /* Mobile-specific styling */
        @media (max-width: 768px) {
            body {
                padding: 0 15px; /* Adjusted padding for mobile devices */
            }
        }
    </style>
</head>
<body>
    <header>
        <h1>Investigate â†’ Research â†’ Prototype â†’ Iterate</h1>
        <p>Focusing on AI Psychology and Alignment </p>
    </header>

    <section>
        <h2>Hello!</h2>
        <p>I'm Miguel! A researcher focusing on AI safety research and synthesis, experimenting on datasets that capture psychology to embed corrigibility traits into AI systems. I'm is exploring innovative methods to make AI systems not just intelligent but also ethically aligned. My safety project tackles the problem of linguistic morphologies in AI systems."</p>
    </section>

    <section>
        <h2>Projects</h2>
            <h3>Reinforcement Learning from Framework Continuums (RLFC)</h3>
            <p><a href="https://www.lesswrong.com/posts/GrxaMeekGKK6WKwmm/rl-for-safety-work-or-just-clever-rl-reinforcement-learning#Reinforced_Learning_from_Framework_Continuums__RLFC_">RLFC</a> is a variant of reinforcement learning, designed to instill human values in Large Language Models (LLMs). It addresses the limitations of existing methods like Reinforcement Learning from Human Feedback (RLHF) by using specific datasets to teach complex patterns. This technique involves a series of interconnected frameworks, progressively enhancing an LLM's grasp of human values. </p>
        </div>
        
       <!-- Links -->
        <div class="project">
            <h2>Links</h2>
            <p>
                <a href="https://www.whitehatstoic.com/">Blog</a><br>
                 <a href="https://linktr.ee/whitehatstoic">LinkðŸŒ³</a><br>
                <a href="https://github.com/migueldeguzman">GitHub</a><br>
                <a href="https://www.linkedin.com/in/miguelito-de-guzman-ai/">LinkedIn</a><br>
                <a href="https://www.lesswrong.com/users/migueldev">Lesswrong</a><br>
                <a href="https://huggingface.co/migueldeguzmandev">Hugging Face</a><br>
                <a href="https://calendly.com/migueldeguzmandev">Set-up a meeting?</a><br>
            </p>
        </div>


        <!-- Add more projects as needed -->

    </section>

    <footer>
        <p></p>
    </footer>
</body>
</html>
