<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Portfolio</title>
    <link rel="icon" href="https://github.com/migueldeguzman/migueldeguzman.github.io/image.png" type="image/x-icon">
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 40px auto;
            max-width: 800px;
            line-height: 1.6;
            padding: 0 20px; /* Add padding to prevent leaning against the edges */
        }

        header {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 10px 0;
        }

        section {
            margin: 20px 0;
        }

        img {
            max-width: 100%;
        }

        footer {
            text-align: center;
            margin-top: 40px;
            font-size: 0.8em;
            color: #777;
        }

        /* Mobile-specific styling */
        @media (max-width: 768px) {
            body {
                padding: 0 15px; /* Adjusted padding for mobile devices */
            }
        }
    </style>
</head>
<body>
    <header>
        <h1>Investigate â†’ Prototype â†’ Iterate</h1>
        <p>Focusing on AI Psychology and Alignment </p>
    </header>

    <section>
        <h2>About Me</h2>
        <p>"Hello! I'm Miguel, My work is dedicated to AI safety and alignment research for the moment, particularly in harnessing the power of psychology to embed corrigibility traits into AI systems. I'm passionate about exploring innovative methods to make AI systems not just intelligent but also ethically aligned, safe, and beneficial for humanity. My recent endeavors in Archetypal Transfer Learning (ATL) aim to enhance the patterns in Large Language Models (LLMs) for better safety protocols. As we venture into a transformative AI landscape, my goal is to ensure that we navigate it with awareness and care."</p>
    </section>

    <section>
        <h2>Projects</h2>
            <h3>Reinforcement Learning from Framework Continuums (RLFC)</h3>
            <p><a href="https://www.lesswrong.com/posts/GrxaMeekGKK6WKwmm/rl-for-safety-work-or-just-clever-rl-reinforcement-learning#Reinforced_Learning_from_Framework_Continuums__RLFC_">RLFC</a><br> is a variant of reinforcement learning, designed to instill human values in Large Language Models (LLMs). It addresses the limitations of existing methods like Reinforcement Learning from Human Feedback (RLHF) by using specific datasets to teach complex patterns. This technique involves a series of interconnected frameworks, progressively enhancing an LLM's grasp of human values. </p>
        </div>
        
       <!-- Links -->
        <div class="project">
            <h2>Links</h2>
            <p>
                <a href="https://www.whitehatstoic.com/">Blog</a><br>
                 <a href="https://linktr.ee/whitehatstoic">LinkðŸŒ³</a><br>
                <a href="https://github.com/migueldeguzman">GitHub</a><br>
                <a href="https://www.linkedin.com/in/miguelito-de-guzman-ai/">LinkedIn</a><br>
                <a href="https://www.lesswrong.com/users/migueldev">Lesswrong</a><br>
                <a href="https://huggingface.co/migueldeguzmandev">Hugging Face</a><br>
                <a href="https://calendly.com/migueldeguzmandev">Set-up a meeting?</a><br>
            </p>
        </div>


        <!-- Add more projects as needed -->

    </section>

    <footer>
        <p>Â© Continously Experimenting, All Rights Reserved, 2023.</p>
    </footer>
</body>
</html>
